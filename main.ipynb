{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings: numerical representations to classify clinical text\n",
    "\n",
    "_Jocelyn Dunstan & Fabián Villena, Centro de Informática Médica y Telemedicina, Universidad de Chile_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #linear algebra\n",
    "import nltk #natural language processing tools\n",
    "import gensim #neural word embedding training\n",
    "import re #regular expressions\n",
    "import logging #verbosity (code telling you what's going on)\n",
    "import csv #data loading\n",
    "import sklearn.ensemble #classifier\n",
    "import sklearn.metrics #performance assessment\n",
    "import sklearn.model_selection #train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading extra data for nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords') # connectors and other non-semantic rich expressions\n",
    "nltk.download('punkt') # sentence tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some verbosity to the embedding training session, i.e., the coding telling you what is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading spanish stopwords list from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'al',\n",
       " 'algo',\n",
       " 'algunas',\n",
       " 'algunos',\n",
       " 'ante',\n",
       " 'antes',\n",
       " 'como',\n",
       " 'con',\n",
       " 'contra',\n",
       " 'cual',\n",
       " 'cuando',\n",
       " 'de',\n",
       " 'del',\n",
       " 'desde',\n",
       " 'donde',\n",
       " 'durante',\n",
       " 'e',\n",
       " 'el',\n",
       " 'ella',\n",
       " 'ellas',\n",
       " 'ellos',\n",
       " 'en',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'eras',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'esas',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'esos',\n",
       " 'esta',\n",
       " 'estaba',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estabas',\n",
       " 'estad',\n",
       " 'estada',\n",
       " 'estadas',\n",
       " 'estado',\n",
       " 'estados',\n",
       " 'estamos',\n",
       " 'estando',\n",
       " 'estar',\n",
       " 'estaremos',\n",
       " 'estará',\n",
       " 'estarán',\n",
       " 'estarás',\n",
       " 'estaré',\n",
       " 'estaréis',\n",
       " 'estaría',\n",
       " 'estaríais',\n",
       " 'estaríamos',\n",
       " 'estarían',\n",
       " 'estarías',\n",
       " 'estas',\n",
       " 'este',\n",
       " 'estemos',\n",
       " 'esto',\n",
       " 'estos',\n",
       " 'estoy',\n",
       " 'estuve',\n",
       " 'estuviera',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuvieras',\n",
       " 'estuvieron',\n",
       " 'estuviese',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estuvieses',\n",
       " 'estuvimos',\n",
       " 'estuviste',\n",
       " 'estuvisteis',\n",
       " 'estuviéramos',\n",
       " 'estuviésemos',\n",
       " 'estuvo',\n",
       " 'está',\n",
       " 'estábamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'estás',\n",
       " 'esté',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estés',\n",
       " 'fue',\n",
       " 'fuera',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fueras',\n",
       " 'fueron',\n",
       " 'fuese',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'fueses',\n",
       " 'fui',\n",
       " 'fuimos',\n",
       " 'fuiste',\n",
       " 'fuisteis',\n",
       " 'fuéramos',\n",
       " 'fuésemos',\n",
       " 'ha',\n",
       " 'habida',\n",
       " 'habidas',\n",
       " 'habido',\n",
       " 'habidos',\n",
       " 'habiendo',\n",
       " 'habremos',\n",
       " 'habrá',\n",
       " 'habrán',\n",
       " 'habrás',\n",
       " 'habré',\n",
       " 'habréis',\n",
       " 'habría',\n",
       " 'habríais',\n",
       " 'habríamos',\n",
       " 'habrían',\n",
       " 'habrías',\n",
       " 'habéis',\n",
       " 'había',\n",
       " 'habíais',\n",
       " 'habíamos',\n",
       " 'habían',\n",
       " 'habías',\n",
       " 'han',\n",
       " 'has',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'haya',\n",
       " 'hayamos',\n",
       " 'hayan',\n",
       " 'hayas',\n",
       " 'hayáis',\n",
       " 'he',\n",
       " 'hemos',\n",
       " 'hube',\n",
       " 'hubiera',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubieras',\n",
       " 'hubieron',\n",
       " 'hubiese',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'hubieses',\n",
       " 'hubimos',\n",
       " 'hubiste',\n",
       " 'hubisteis',\n",
       " 'hubiéramos',\n",
       " 'hubiésemos',\n",
       " 'hubo',\n",
       " 'la',\n",
       " 'las',\n",
       " 'le',\n",
       " 'les',\n",
       " 'lo',\n",
       " 'los',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'mucho',\n",
       " 'muchos',\n",
       " 'muy',\n",
       " 'más',\n",
       " 'mí',\n",
       " 'mía',\n",
       " 'mías',\n",
       " 'mío',\n",
       " 'míos',\n",
       " 'nada',\n",
       " 'ni',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nosotras',\n",
       " 'nosotros',\n",
       " 'nuestra',\n",
       " 'nuestras',\n",
       " 'nuestro',\n",
       " 'nuestros',\n",
       " 'o',\n",
       " 'os',\n",
       " 'otra',\n",
       " 'otras',\n",
       " 'otro',\n",
       " 'otros',\n",
       " 'para',\n",
       " 'pero',\n",
       " 'poco',\n",
       " 'por',\n",
       " 'porque',\n",
       " 'que',\n",
       " 'quien',\n",
       " 'quienes',\n",
       " 'qué',\n",
       " 'se',\n",
       " 'sea',\n",
       " 'seamos',\n",
       " 'sean',\n",
       " 'seas',\n",
       " 'sentid',\n",
       " 'sentida',\n",
       " 'sentidas',\n",
       " 'sentido',\n",
       " 'sentidos',\n",
       " 'seremos',\n",
       " 'será',\n",
       " 'serán',\n",
       " 'serás',\n",
       " 'seré',\n",
       " 'seréis',\n",
       " 'sería',\n",
       " 'seríais',\n",
       " 'seríamos',\n",
       " 'serían',\n",
       " 'serías',\n",
       " 'seáis',\n",
       " 'siente',\n",
       " 'sin',\n",
       " 'sintiendo',\n",
       " 'sobre',\n",
       " 'sois',\n",
       " 'somos',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'su',\n",
       " 'sus',\n",
       " 'suya',\n",
       " 'suyas',\n",
       " 'suyo',\n",
       " 'suyos',\n",
       " 'sí',\n",
       " 'también',\n",
       " 'tanto',\n",
       " 'te',\n",
       " 'tendremos',\n",
       " 'tendrá',\n",
       " 'tendrán',\n",
       " 'tendrás',\n",
       " 'tendré',\n",
       " 'tendréis',\n",
       " 'tendría',\n",
       " 'tendríais',\n",
       " 'tendríamos',\n",
       " 'tendrían',\n",
       " 'tendrías',\n",
       " 'tened',\n",
       " 'tenemos',\n",
       " 'tenga',\n",
       " 'tengamos',\n",
       " 'tengan',\n",
       " 'tengas',\n",
       " 'tengo',\n",
       " 'tengáis',\n",
       " 'tenida',\n",
       " 'tenidas',\n",
       " 'tenido',\n",
       " 'tenidos',\n",
       " 'teniendo',\n",
       " 'tenéis',\n",
       " 'tenía',\n",
       " 'teníais',\n",
       " 'teníamos',\n",
       " 'tenían',\n",
       " 'tenías',\n",
       " 'ti',\n",
       " 'tiene',\n",
       " 'tienen',\n",
       " 'tienes',\n",
       " 'todo',\n",
       " 'todos',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'tuve',\n",
       " 'tuviera',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuvieras',\n",
       " 'tuvieron',\n",
       " 'tuviese',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'tuvieses',\n",
       " 'tuvimos',\n",
       " 'tuviste',\n",
       " 'tuvisteis',\n",
       " 'tuviéramos',\n",
       " 'tuviésemos',\n",
       " 'tuvo',\n",
       " 'tuya',\n",
       " 'tuyas',\n",
       " 'tuyo',\n",
       " 'tuyos',\n",
       " 'tú',\n",
       " 'un',\n",
       " 'una',\n",
       " 'uno',\n",
       " 'unos',\n",
       " 'vosostras',\n",
       " 'vosostros',\n",
       " 'vuestra',\n",
       " 'vuestras',\n",
       " 'vuestro',\n",
       " 'vuestros',\n",
       " 'y',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'él',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some self-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizer(text): #normalizes a given string to lowercase and changes all vowels to their base form\n",
    "    text = text.lower() #string lowering\n",
    "    text = re.sub(r'[^A-Za-zñáéíóú]', ' ', text) #replaces every punctuation with a space\n",
    "    text = re.sub('á', 'a', text) #replaces special vowels to their base forms\n",
    "    text = re.sub('é', 'e', text)\n",
    "    text = re.sub('í', 'i', text)\n",
    "    text = re.sub('ó', 'o', text)\n",
    "    text = re.sub('ú', 'u', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer(text, model): #returns a vector representation from a list of words and a given model\n",
    "    vectors = []\n",
    "    for i in text:\n",
    "        try:\n",
    "            vectors.append(model.wv[i])\n",
    "        except:\n",
    "            pass\n",
    "    return(np.mean(vectors,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the corpus dataset and taking each line to an array of sentences. We are using the whole Aysen's waiting list as a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open('corpus.txt', encoding='utf-8') as file: #use utf-8 to preserve special characters\n",
    "    for line in file:\n",
    "        sentences.append((line.rstrip())) #use strip to remove \\n (newline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking every sentence and normalizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_sentences = [normalizer(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing every sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in normalized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "without_stopwords_sentences = []\n",
    "for sentence in tokenized_sentences:\n",
    "    without_stopwords_sentence = [word for word in sentence if word not in stopwords]\n",
    "    without_stopwords_sentences.append(without_stopwords_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural word embeddings word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-15 10:27:44,803 : INFO : collecting all words and their counts\n",
      "2019-06-15 10:27:44,806 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-15 10:27:44,839 : INFO : PROGRESS: at sentence #10000, processed 26393 words, keeping 2174 word types\n",
      "2019-06-15 10:27:44,861 : INFO : PROGRESS: at sentence #20000, processed 53049 words, keeping 2795 word types\n",
      "2019-06-15 10:27:44,883 : INFO : PROGRESS: at sentence #30000, processed 80100 words, keeping 3189 word types\n",
      "2019-06-15 10:27:44,908 : INFO : PROGRESS: at sentence #40000, processed 107473 words, keeping 3528 word types\n",
      "2019-06-15 10:27:44,931 : INFO : PROGRESS: at sentence #50000, processed 134970 words, keeping 3783 word types\n",
      "2019-06-15 10:27:44,956 : INFO : PROGRESS: at sentence #60000, processed 162957 words, keeping 3986 word types\n",
      "2019-06-15 10:27:44,985 : INFO : PROGRESS: at sentence #70000, processed 191225 words, keeping 4140 word types\n",
      "2019-06-15 10:27:45,024 : INFO : PROGRESS: at sentence #80000, processed 219291 words, keeping 4298 word types\n",
      "2019-06-15 10:27:45,070 : INFO : PROGRESS: at sentence #90000, processed 247027 words, keeping 4430 word types\n",
      "2019-06-15 10:27:45,099 : INFO : PROGRESS: at sentence #100000, processed 276646 words, keeping 4559 word types\n",
      "2019-06-15 10:27:45,146 : INFO : PROGRESS: at sentence #110000, processed 305141 words, keeping 4645 word types\n",
      "2019-06-15 10:27:45,206 : INFO : PROGRESS: at sentence #120000, processed 335101 words, keeping 4733 word types\n",
      "2019-06-15 10:27:45,255 : INFO : PROGRESS: at sentence #130000, processed 365193 words, keeping 4831 word types\n",
      "2019-06-15 10:27:45,286 : INFO : PROGRESS: at sentence #140000, processed 394265 words, keeping 4894 word types\n",
      "2019-06-15 10:27:45,314 : INFO : PROGRESS: at sentence #150000, processed 423907 words, keeping 4974 word types\n",
      "2019-06-15 10:27:45,345 : INFO : PROGRESS: at sentence #160000, processed 454862 words, keeping 5022 word types\n",
      "2019-06-15 10:27:45,389 : INFO : PROGRESS: at sentence #170000, processed 489295 words, keeping 5075 word types\n",
      "2019-06-15 10:27:45,419 : INFO : PROGRESS: at sentence #180000, processed 523867 words, keeping 5108 word types\n",
      "2019-06-15 10:27:45,461 : INFO : PROGRESS: at sentence #190000, processed 559713 words, keeping 5127 word types\n",
      "2019-06-15 10:27:45,490 : INFO : PROGRESS: at sentence #200000, processed 588689 words, keeping 5147 word types\n",
      "2019-06-15 10:27:45,515 : INFO : PROGRESS: at sentence #210000, processed 608952 words, keeping 5147 word types\n",
      "2019-06-15 10:27:45,540 : INFO : collected 5150 word types from a corpus of 625155 raw words and 217897 sentences\n",
      "2019-06-15 10:27:45,543 : INFO : Loading a fresh vocabulary\n",
      "2019-06-15 10:27:45,566 : INFO : min_count=5 retains 2782 unique words (54% of original 5150, drops 2368)\n",
      "2019-06-15 10:27:45,568 : INFO : min_count=5 leaves 620802 word corpus (99% of original 625155, drops 4353)\n",
      "2019-06-15 10:27:45,593 : INFO : deleting the raw counts dictionary of 5150 items\n",
      "2019-06-15 10:27:45,596 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-06-15 10:27:45,597 : INFO : downsampling leaves estimated 414882 word corpus (66.8% of prior 620802)\n",
      "2019-06-15 10:27:45,617 : INFO : estimated required memory for 2782 words and 100 dimensions: 3616600 bytes\n",
      "2019-06-15 10:27:45,619 : INFO : resetting layer weights\n",
      "2019-06-15 10:27:45,720 : INFO : training model with 3 workers on 2782 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-15 10:27:46,728 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-15 10:27:46,733 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-15 10:27:46,742 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 411262 words/s, in_qsize 0, out_qsize 1\n",
      "2019-06-15 10:27:46,743 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-15 10:27:46,744 : INFO : EPOCH - 1 : training on 625155 raw words (414692 effective words) took 1.0s, 410243 effective words/s\n",
      "2019-06-15 10:27:47,813 : INFO : EPOCH 2 - PROGRESS: at 51.25% examples, 228911 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-15 10:27:48,365 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-15 10:27:48,370 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-15 10:27:48,372 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-15 10:27:48,374 : INFO : EPOCH - 2 : training on 625155 raw words (414954 effective words) took 1.6s, 257618 effective words/s\n",
      "2019-06-15 10:27:49,401 : INFO : EPOCH 3 - PROGRESS: at 79.47% examples, 364655 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-15 10:27:49,505 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-15 10:27:49,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-15 10:27:49,510 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-15 10:27:49,512 : INFO : EPOCH - 3 : training on 625155 raw words (415034 effective words) took 1.1s, 369208 effective words/s\n",
      "2019-06-15 10:27:50,418 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-15 10:27:50,423 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-15 10:27:50,425 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-15 10:27:50,426 : INFO : EPOCH - 4 : training on 625155 raw words (415184 effective words) took 0.9s, 460482 effective words/s\n",
      "2019-06-15 10:27:51,452 : INFO : EPOCH 5 - PROGRESS: at 82.08% examples, 376510 words/s, in_qsize 4, out_qsize 0\n",
      "2019-06-15 10:27:51,641 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-15 10:27:51,645 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-15 10:27:51,661 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-15 10:27:51,665 : INFO : EPOCH - 5 : training on 625155 raw words (414900 effective words) took 1.2s, 339769 effective words/s\n",
      "2019-06-15 10:27:51,669 : INFO : training on a 3125775 raw words (2074764 effective words) took 5.9s, 348936 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(without_stopwords_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the most similar words to a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-15 10:27:51,692 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('arcos', 0.9807857275009155),\n",
       " ('posicion', 0.9750917553901672),\n",
       " ('dentarios', 0.9700753688812256),\n",
       " ('relacion', 0.9640514850616455),\n",
       " ('evidentes', 0.9584258794784546),\n",
       " ('anomalias', 0.9575490951538086),\n",
       " ('dentofaciales', 0.9548975229263306),\n",
       " ('maxilobasilar', 0.9412839412689209),\n",
       " ('tamaño', 0.9192438721656799),\n",
       " ('incluso', 0.9082833528518677)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('diente')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your own words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: Can we classify the medical specialty from the diagnosis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the raw dataset and extracting the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics = [] #classifier raw features\n",
    "specialties = [] #classifier raw labels\n",
    "with open('data.csv', encoding='utf-8') as file:\n",
    "    data = csv.DictReader(file)\n",
    "    for row in data:\n",
    "        diagnostics.append(row['diagnostic'])\n",
    "        specialties.append(row['specialty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_normalized = [normalizer(diagnostic) for diagnostic in diagnostics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_tokenized = [nltk.word_tokenize(diagnostic) for diagnostic in diagnostics_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_wihout_stopwords = []\n",
    "for diagnostic in diagnostics_tokenized:\n",
    "    diagnostic_wihout_stopwords = [word for word in diagnostic if word not in stopwords]\n",
    "    diagnostics_wihout_stopwords.append(diagnostic_wihout_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty matrix to store the encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_matrix = np.zeros((len(diagnostics_wihout_stopwords), len(model.wv['cancer'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the matrix with the vectorized diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for i,diagnostic in enumerate(diagnostics_wihout_stopwords):\n",
    "    vector = vectorizer(diagnostic,model)\n",
    "    diagnostics_matrix[i,] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty vector to store the encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specialties_vector = np.zeros((len(specialties), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the vector with the encoded specialties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,specialty in enumerate(specialties): #OFTALMOLOGIA is encoded as a 1.0 and TRAUMATOLOGIA as a 2.0\n",
    "    if specialty == 'OFTALMOLOGIA':\n",
    "        specialties_vector[i,] = 1\n",
    "    else:\n",
    "        specialties_vector[i,] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating the encoded features ans labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_matrix = np.concatenate([diagnostics_matrix,specialties_vector], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing NAs from the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_matrix_without_nan = data_matrix[~np.isnan(data_matrix).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into training and testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_train, diagnostics_test, specialties_train, specialties_test = sklearn.model_selection.train_test_split(\n",
    "    data_matrix_without_nan[:,:100],\n",
    "    data_matrix_without_nan[:,100],\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = sklearn.ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(diagnostics_train,specialties_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions over the testing subset and measuring their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.98      0.87      5619\n",
      "         2.0       0.98      0.82      0.89      8536\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     14155\n",
      "   macro avg       0.88      0.90      0.88     14155\n",
      "weighted avg       0.90      0.88      0.88     14155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(diagnostics_test)\n",
    "print(sklearn.metrics.classification_report(predictions, specialties_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision = TP/(TP+FP) # accuracy of positive predictions \n",
    "- Recall = TP/(TP+FN) # sensitivity or true positive rate\n",
    "- F1-score = 2 x (precision x recall)/(precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual testing of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialtyClassifier(diagnostic):\n",
    "    try:\n",
    "        stringNorm = normalizer(diagnostic)\n",
    "        stringTokenized = nltk.word_tokenize(stringNorm)\n",
    "        stringVec = vectorizer(stringTokenized,model)\n",
    "        result = classifier.predict(stringVec.reshape(1, -1))[0]\n",
    "        prob = classifier.predict_proba(stringVec.reshape(1, -1))[0]\n",
    "        if result==1.0: \n",
    "            result = \"Oftalmología\"\n",
    "        if result==2.0: \n",
    "            result = 'Traumatología'\n",
    "        prob_oft = prob[0]\n",
    "        prob_tra = prob[1]\n",
    "        return('''\n",
    "        \n",
    "        Diagnostic: {diagnostic}\n",
    "        ---\n",
    "        Predicted Specialty:\\t{result}\n",
    "        ---\n",
    "        Probabilities\n",
    "        Oftalmología:\\t{prob_oft}\n",
    "        Traumatología:\\t{prob_tra}\n",
    "        \n",
    "        '''.format(\n",
    "            diagnostic = diagnostic,\n",
    "            result = result,\n",
    "            prob_oft = prob_oft,\n",
    "            prob_tra = prob_tra,\n",
    "        )\n",
    "              )\n",
    "    except:\n",
    "        return('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        \n",
      "        Diagnostic: vicio de refracción\n",
      "        ---\n",
      "        Predicted Specialty:\tOftalmología\n",
      "        ---\n",
      "        Probabilities\n",
      "        Oftalmología:\t1.0\n",
      "        Traumatología:\t0.0\n",
      "        \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(specialtyClassifier('vicio de refracción'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        \n",
      "        Diagnostic: fractura de órbita\n",
      "        ---\n",
      "        Predicted Specialty:\tTraumatología\n",
      "        ---\n",
      "        Probabilities\n",
      "        Oftalmología:\t0.4\n",
      "        Traumatología:\t0.6\n",
      "        \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(specialtyClassifier('fractura de órbita'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your own!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
