{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import logging\n",
    "import csv\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^A-Za-zñáéíóú]', ' ', text)\n",
    "    text = re.sub('á', 'a', text)\n",
    "    text = re.sub('é', 'e', text)\n",
    "    text = re.sub('í', 'i', text)\n",
    "    text = re.sub('ó', 'o', text)\n",
    "    text = re.sub('ú', 'u', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text, model):\n",
    "    vectors = []\n",
    "    for i in text:\n",
    "        try:\n",
    "            vectors.append(model.wv[i])\n",
    "        except:\n",
    "            pass\n",
    "    return(np.mean(vectors,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open('corpus.txt', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        sentences.append((line.rstrip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_sentences = [normalizer(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in normalized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_stopwords_sentences = []\n",
    "for sentence in tokenized_sentences:\n",
    "    without_stopwords_sentence = [word for word in sentence if word not in stopwords]\n",
    "    without_stopwords_sentences.append(without_stopwords_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-03 12:25:52,135 : INFO : collecting all words and their counts\n",
      "2019-06-03 12:25:52,138 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-03 12:25:52,175 : INFO : PROGRESS: at sentence #10000, processed 26393 words, keeping 2174 word types\n",
      "2019-06-03 12:25:52,201 : INFO : PROGRESS: at sentence #20000, processed 53049 words, keeping 2795 word types\n",
      "2019-06-03 12:25:52,237 : INFO : PROGRESS: at sentence #30000, processed 80100 words, keeping 3189 word types\n",
      "2019-06-03 12:25:52,266 : INFO : PROGRESS: at sentence #40000, processed 107473 words, keeping 3528 word types\n",
      "2019-06-03 12:25:52,281 : INFO : PROGRESS: at sentence #50000, processed 134970 words, keeping 3783 word types\n",
      "2019-06-03 12:25:52,303 : INFO : PROGRESS: at sentence #60000, processed 162957 words, keeping 3986 word types\n",
      "2019-06-03 12:25:52,320 : INFO : PROGRESS: at sentence #70000, processed 191225 words, keeping 4140 word types\n",
      "2019-06-03 12:25:52,337 : INFO : PROGRESS: at sentence #80000, processed 219291 words, keeping 4298 word types\n",
      "2019-06-03 12:25:52,358 : INFO : PROGRESS: at sentence #90000, processed 247027 words, keeping 4430 word types\n",
      "2019-06-03 12:25:52,374 : INFO : PROGRESS: at sentence #100000, processed 276646 words, keeping 4559 word types\n",
      "2019-06-03 12:25:52,388 : INFO : PROGRESS: at sentence #110000, processed 305141 words, keeping 4645 word types\n",
      "2019-06-03 12:25:52,410 : INFO : PROGRESS: at sentence #120000, processed 335101 words, keeping 4733 word types\n",
      "2019-06-03 12:25:52,430 : INFO : PROGRESS: at sentence #130000, processed 365193 words, keeping 4831 word types\n",
      "2019-06-03 12:25:52,454 : INFO : PROGRESS: at sentence #140000, processed 394265 words, keeping 4894 word types\n",
      "2019-06-03 12:25:52,485 : INFO : PROGRESS: at sentence #150000, processed 423907 words, keeping 4974 word types\n",
      "2019-06-03 12:25:52,516 : INFO : PROGRESS: at sentence #160000, processed 454862 words, keeping 5022 word types\n",
      "2019-06-03 12:25:52,612 : INFO : PROGRESS: at sentence #170000, processed 489295 words, keeping 5075 word types\n",
      "2019-06-03 12:25:52,668 : INFO : PROGRESS: at sentence #180000, processed 523867 words, keeping 5108 word types\n",
      "2019-06-03 12:25:52,691 : INFO : PROGRESS: at sentence #190000, processed 559713 words, keeping 5127 word types\n",
      "2019-06-03 12:25:52,703 : INFO : PROGRESS: at sentence #200000, processed 588689 words, keeping 5147 word types\n",
      "2019-06-03 12:25:52,719 : INFO : PROGRESS: at sentence #210000, processed 608952 words, keeping 5147 word types\n",
      "2019-06-03 12:25:52,729 : INFO : collected 5150 word types from a corpus of 625155 raw words and 217897 sentences\n",
      "2019-06-03 12:25:52,730 : INFO : Loading a fresh vocabulary\n",
      "2019-06-03 12:25:52,743 : INFO : min_count=5 retains 2782 unique words (54% of original 5150, drops 2368)\n",
      "2019-06-03 12:25:52,745 : INFO : min_count=5 leaves 620802 word corpus (99% of original 625155, drops 4353)\n",
      "2019-06-03 12:25:52,776 : INFO : deleting the raw counts dictionary of 5150 items\n",
      "2019-06-03 12:25:52,778 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-06-03 12:25:52,780 : INFO : downsampling leaves estimated 414882 word corpus (66.8% of prior 620802)\n",
      "2019-06-03 12:25:52,802 : INFO : estimated required memory for 2782 words and 100 dimensions: 3616600 bytes\n",
      "2019-06-03 12:25:52,803 : INFO : resetting layer weights\n",
      "2019-06-03 12:25:52,882 : INFO : training model with 3 workers on 2782 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-03 12:25:53,855 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 12:25:53,857 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 12:25:53,859 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 12:25:53,861 : INFO : EPOCH - 1 : training on 625155 raw words (414869 effective words) took 1.0s, 428637 effective words/s\n",
      "2019-06-03 12:25:54,928 : INFO : EPOCH 2 - PROGRESS: at 41.63% examples, 186618 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-03 12:25:55,864 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 12:25:55,874 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 12:25:55,877 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 12:25:55,880 : INFO : EPOCH - 2 : training on 625155 raw words (414816 effective words) took 2.0s, 208186 effective words/s\n",
      "2019-06-03 12:25:56,906 : INFO : EPOCH 3 - PROGRESS: at 82.08% examples, 376332 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-03 12:25:56,974 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 12:25:56,977 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 12:25:56,978 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 12:25:56,982 : INFO : EPOCH - 3 : training on 625155 raw words (414707 effective words) took 1.1s, 382505 effective words/s\n",
      "2019-06-03 12:25:58,035 : INFO : EPOCH 4 - PROGRESS: at 49.58% examples, 225502 words/s, in_qsize 4, out_qsize 1\n",
      "2019-06-03 12:25:58,554 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 12:25:58,561 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 12:25:58,563 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 12:25:58,564 : INFO : EPOCH - 4 : training on 625155 raw words (414810 effective words) took 1.6s, 265712 effective words/s\n",
      "2019-06-03 12:25:59,611 : INFO : EPOCH 5 - PROGRESS: at 60.40% examples, 279014 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-03 12:26:00,401 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 12:26:00,412 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 12:26:00,415 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 12:26:00,418 : INFO : EPOCH - 5 : training on 625155 raw words (414483 effective words) took 1.8s, 225103 effective words/s\n",
      "2019-06-03 12:26:00,423 : INFO : training on a 3125775 raw words (2073685 effective words) took 7.5s, 275027 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(without_stopwords_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-06-03 12:26:00,476 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('metastasis', 0.885179877281189),\n",
       " ('avanzado', 0.8725283145904541),\n",
       " ('adenocarcinoma', 0.8516897559165955),\n",
       " ('adenoma', 0.8304771184921265),\n",
       " ('infiltrante', 0.8269941210746765),\n",
       " ('cervix', 0.8217967748641968),\n",
       " ('atipicas', 0.8022570013999939),\n",
       " ('fallecido', 0.7991020679473877),\n",
       " ('significado', 0.7990065813064575),\n",
       " ('indeterminado', 0.7948452234268188)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics = []\n",
    "specialties = []\n",
    "with open('data.csv', encoding='utf-8') as file:\n",
    "    data = csv.DictReader(file)\n",
    "    for row in data:\n",
    "        diagnostics.append(row['diagnostic'])\n",
    "        specialties.append(row['specialty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_normalized = [normalizer(diagnostic) for diagnostic in diagnostics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_tokenized = [nltk.word_tokenize(diagnostic) for diagnostic in diagnostics_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_wihout_stopwords = []\n",
    "for diagnostic in diagnostics_tokenized:\n",
    "    diagnostic_wihout_stopwords = [word for word in diagnostic if word not in stopwords]\n",
    "    diagnostics_wihout_stopwords.append(diagnostic_wihout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "diagnostics_matrix = np.zeros((len(diagnostics_wihout_stopwords), len(model['cancer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for i,diagnostic in enumerate(diagnostics_wihout_stopwords):\n",
    "    vector = vectorizer(diagnostic,model)\n",
    "    diagnostics_matrix[i,] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialties_vector = np.zeros((len(specialties), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,specialty in enumerate(specialties):\n",
    "    if specialty == 'OFTALMOLOGIA':\n",
    "        specialties_vector[i,] = 1\n",
    "    else:\n",
    "        specialties_vector[i,] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.concatenate([diagnostics_matrix,specialties_vector], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_without_nan = data_matrix[~np.isnan(data_matrix).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_train, diagnostics_test, specialties_train, specialties_test = sklearn.model_selection.train_test_split(\n",
    "    data_matrix_without_nan[:,:100],\n",
    "    data_matrix_without_nan[:,100],\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(diagnostics_train,specialties_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.98      0.87      5609\n",
      "         2.0       0.99      0.82      0.89      8546\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     14155\n",
      "   macro avg       0.88      0.90      0.88     14155\n",
      "weighted avg       0.90      0.88      0.88     14155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(diagnostics_test)\n",
    "print(sklearn.metrics.classification_report(predictions, specialties_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialtyClassifier(diagnostic):\n",
    "    try:\n",
    "        stringNorm = normalizer(diagnostic)\n",
    "        stringTokenized = nltk.word_tokenize(stringNorm)\n",
    "        stringVec = vectorizer(stringTokenized,model)\n",
    "        result = classifier.predict(stringVec.reshape(1, -1))[0]\n",
    "        return(result)\n",
    "    except:\n",
    "        return(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specialtyClassifier('vicio de refracción')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
