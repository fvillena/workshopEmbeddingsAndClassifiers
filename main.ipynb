{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings and Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #linear algebra\n",
    "import nltk #natural language processing tools\n",
    "import gensim #neural word embedding training\n",
    "import re #regular expressions\n",
    "import logging #verbosity\n",
    "import csv #data loading\n",
    "import sklearn.ensemble #classifier\n",
    "import sklearn.metrics #performance assessment\n",
    "import sklearn.model_selection #train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading extra data for nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some verbosity to the embedding training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading spanish stopwords list from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(text): #normalizes a given string to lowercase and changes all vowels to their base form\n",
    "    text = text.lower() #string lowering\n",
    "    text = re.sub(r'[^A-Za-zñáéíóú]', ' ', text) #replaces every punctuation with a space\n",
    "    text = re.sub('á', 'a', text) #replaces special vowels to their base forms\n",
    "    text = re.sub('é', 'e', text)\n",
    "    text = re.sub('í', 'i', text)\n",
    "    text = re.sub('ó', 'o', text)\n",
    "    text = re.sub('ú', 'u', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text, model): #returns a vector representation from a list of words and a given model\n",
    "    vectors = []\n",
    "    for i in text:\n",
    "        try:\n",
    "            vectors.append(model.wv[i])\n",
    "        except:\n",
    "            pass\n",
    "    return(np.mean(vectors,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the corpus dataset and taking each line to an array of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open('corpus.txt', encoding='utf-8') as file: #use utf-8 to preserve special characters\n",
    "    for line in file:\n",
    "        sentences.append((line.rstrip())) #use strip to remove \\n (newline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking every sentence and normalizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_sentences = [normalizer(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing every sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in normalized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_stopwords_sentences = []\n",
    "for sentence in tokenized_sentences:\n",
    "    without_stopwords_sentence = [word for word in sentence if word not in stopwords]\n",
    "    without_stopwords_sentences.append(without_stopwords_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural word embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-03 12:53:26,507 : INFO : collecting all words and their counts\n",
      "2019-06-03 12:53:26,510 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-03 12:53:26,554 : INFO : PROGRESS: at sentence #10000, processed 26393 words, keeping 2174 word types\n",
      "2019-06-03 12:53:26,583 : INFO : PROGRESS: at sentence #20000, processed 53049 words, keeping 2795 word types\n",
      "2019-06-03 12:53:26,607 : INFO : PROGRESS: at sentence #30000, processed 80100 words, keeping 3189 word types\n",
      "2019-06-03 12:53:26,667 : INFO : PROGRESS: at sentence #40000, processed 107473 words, keeping 3528 word types\n",
      "2019-06-03 12:53:26,700 : INFO : PROGRESS: at sentence #50000, processed 134970 words, keeping 3783 word types\n",
      "2019-06-03 12:53:26,748 : INFO : PROGRESS: at sentence #60000, processed 162957 words, keeping 3986 word types\n",
      "2019-06-03 12:53:26,778 : INFO : PROGRESS: at sentence #70000, processed 191225 words, keeping 4140 word types\n",
      "2019-06-03 12:53:26,805 : INFO : PROGRESS: at sentence #80000, processed 219291 words, keeping 4298 word types\n",
      "2019-06-03 12:53:26,831 : INFO : PROGRESS: at sentence #90000, processed 247027 words, keeping 4430 word types\n",
      "2019-06-03 12:53:26,861 : INFO : PROGRESS: at sentence #100000, processed 276646 words, keeping 4559 word types\n",
      "2019-06-03 12:53:26,889 : INFO : PROGRESS: at sentence #110000, processed 305141 words, keeping 4645 word types\n",
      "2019-06-03 12:53:26,915 : INFO : PROGRESS: at sentence #120000, processed 335101 words, keeping 4733 word types\n",
      "2019-06-03 12:53:26,945 : INFO : PROGRESS: at sentence #130000, processed 365193 words, keeping 4831 word types\n",
      "2019-06-03 12:53:26,975 : INFO : PROGRESS: at sentence #140000, processed 394265 words, keeping 4894 word types\n",
      "2019-06-03 12:53:27,015 : INFO : PROGRESS: at sentence #150000, processed 423907 words, keeping 4974 word types\n",
      "2019-06-03 12:53:27,030 : INFO : PROGRESS: at sentence #160000, processed 454862 words, keeping 5022 word types\n",
      "2019-06-03 12:53:27,051 : INFO : PROGRESS: at sentence #170000, processed 489295 words, keeping 5075 word types\n",
      "2019-06-03 12:53:27,087 : INFO : PROGRESS: at sentence #180000, processed 523867 words, keeping 5108 word types\n",
      "2019-06-03 12:53:27,103 : INFO : PROGRESS: at sentence #190000, processed 559713 words, keeping 5127 word types\n",
      "2019-06-03 12:53:27,120 : INFO : PROGRESS: at sentence #200000, processed 588689 words, keeping 5147 word types\n",
      "2019-06-03 12:53:27,145 : INFO : PROGRESS: at sentence #210000, processed 608952 words, keeping 5147 word types\n",
      "2019-06-03 12:53:27,186 : INFO : collected 5150 word types from a corpus of 625155 raw words and 217897 sentences\n",
      "2019-06-03 12:53:27,189 : INFO : Loading a fresh vocabulary\n",
      "2019-06-03 12:53:27,209 : INFO : min_count=5 retains 2782 unique words (54% of original 5150, drops 2368)\n",
      "2019-06-03 12:53:27,211 : INFO : min_count=5 leaves 620802 word corpus (99% of original 625155, drops 4353)\n",
      "2019-06-03 12:53:27,277 : INFO : deleting the raw counts dictionary of 5150 items\n",
      "2019-06-03 12:53:27,280 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-06-03 12:53:27,282 : INFO : downsampling leaves estimated 414882 word corpus (66.8% of prior 620802)\n",
      "2019-06-03 12:53:27,312 : INFO : estimated required memory for 2782 words and 100 dimensions: 3616600 bytes\n",
      "2019-06-03 12:53:27,315 : INFO : resetting layer weights\n",
      "2019-06-03 12:53:27,388 : INFO : training model with 3 workers on 2782 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-03 12:53:28,088 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 12:53:28,090 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 12:53:28,091 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 12:53:28,092 : INFO : EPOCH - 1 : training on 625155 raw words (414879 effective words) took 0.7s, 599229 effective words/s\n",
      "2019-06-03 12:53:28,890 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 12:53:28,893 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 12:53:28,894 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 12:53:28,896 : INFO : EPOCH - 2 : training on 625155 raw words (414586 effective words) took 0.8s, 522585 effective words/s\n",
      "2019-06-03 12:53:29,913 : INFO : EPOCH 3 - PROGRESS: at 75.35% examples, 349720 words/s, in_qsize 5, out_qsize 2\n",
      "2019-06-03 12:53:30,302 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 12:53:30,307 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 12:53:30,312 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 12:53:30,315 : INFO : EPOCH - 3 : training on 625155 raw words (414729 effective words) took 1.4s, 294700 effective words/s\n",
      "2019-06-03 12:53:31,627 : INFO : EPOCH 4 - PROGRESS: at 22.12% examples, 101272 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-03 12:53:32,687 : INFO : EPOCH 4 - PROGRESS: at 83.34% examples, 186438 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-03 12:53:32,767 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 12:53:32,770 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 12:53:32,773 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 12:53:32,774 : INFO : EPOCH - 4 : training on 625155 raw words (414798 effective words) took 2.1s, 193060 effective words/s\n",
      "2019-06-03 12:53:33,869 : INFO : EPOCH 5 - PROGRESS: at 60.46% examples, 280275 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-03 12:53:34,241 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 12:53:34,246 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 12:53:34,252 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 12:53:34,253 : INFO : EPOCH - 5 : training on 625155 raw words (414845 effective words) took 1.4s, 294058 effective words/s\n",
      "2019-06-03 12:53:34,257 : INFO : training on a 3125775 raw words (2073837 effective words) took 6.9s, 301919 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(without_stopwords_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the most similar words to a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-03 12:53:34,272 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('metastasis', 0.888931393623352),\n",
       " ('avanzado', 0.871686577796936),\n",
       " ('adenoma', 0.8714392185211182),\n",
       " ('adenocarcinoma', 0.8571346998214722),\n",
       " ('infiltrante', 0.8364208340644836),\n",
       " ('cervix', 0.8173503279685974),\n",
       " ('indeterminado', 0.8064922094345093),\n",
       " ('oseas', 0.804007887840271),\n",
       " ('evacuador', 0.802149772644043),\n",
       " ('significado', 0.8013938069343567)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('cancer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the raw dataset and extracting the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics = [] #classifier raw features\n",
    "specialties = [] #classifier raw labels\n",
    "with open('data.csv', encoding='utf-8') as file:\n",
    "    data = csv.DictReader(file)\n",
    "    for row in data:\n",
    "        diagnostics.append(row['diagnostic'])\n",
    "        specialties.append(row['specialty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_normalized = [normalizer(diagnostic) for diagnostic in diagnostics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_tokenized = [nltk.word_tokenize(diagnostic) for diagnostic in diagnostics_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_wihout_stopwords = []\n",
    "for diagnostic in diagnostics_tokenized:\n",
    "    diagnostic_wihout_stopwords = [word for word in diagnostic if word not in stopwords]\n",
    "    diagnostics_wihout_stopwords.append(diagnostic_wihout_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty matrix to store the encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_matrix = np.zeros((len(diagnostics_wihout_stopwords), len(model.wv['cancer'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the matrix with the vectorized diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for i,diagnostic in enumerate(diagnostics_wihout_stopwords):\n",
    "    vector = vectorizer(diagnostic,model)\n",
    "    diagnostics_matrix[i,] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty vector to store the encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialties_vector = np.zeros((len(specialties), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the vector with the encoded specialties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,specialty in enumerate(specialties): #OFTALMOLOGIA is encoded as a 1.0 and TRAUMATOLOGIA as a 2.0\n",
    "    if specialty == 'OFTALMOLOGIA':\n",
    "        specialties_vector[i,] = 1\n",
    "    else:\n",
    "        specialties_vector[i,] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating the encoded features ans labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.concatenate([diagnostics_matrix,specialties_vector], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing NAs from the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_without_nan = data_matrix[~np.isnan(data_matrix).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into training and testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_train, diagnostics_test, specialties_train, specialties_test = sklearn.model_selection.train_test_split(\n",
    "    data_matrix_without_nan[:,:100],\n",
    "    data_matrix_without_nan[:,100],\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ville\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(diagnostics_train,specialties_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions over the testing subset and measuring their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.98      0.87      5614\n",
      "         2.0       0.98      0.82      0.89      8541\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     14155\n",
      "   macro avg       0.88      0.90      0.88     14155\n",
      "weighted avg       0.90      0.88      0.88     14155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(diagnostics_test)\n",
    "print(sklearn.metrics.classification_report(predictions, specialties_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual testing of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialtyClassifier(diagnostic):\n",
    "    try:\n",
    "        stringNorm = normalizer(diagnostic)\n",
    "        stringTokenized = nltk.word_tokenize(stringNorm)\n",
    "        stringVec = vectorizer(stringTokenized,model)\n",
    "        result = classifier.predict(stringVec.reshape(1, -1))[0]\n",
    "        return(result)\n",
    "    except:\n",
    "        return(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specialtyClassifier('vicio de refracción')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
