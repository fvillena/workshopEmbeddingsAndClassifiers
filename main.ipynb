{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings: numerical representations to classify clinical text\n",
    "\n",
    "### Jocelyn Dunstan y Fabián Villena, CIMT, Uchile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #linear algebra\n",
    "import nltk #natural language processing tools\n",
    "import gensim #neural word embedding training\n",
    "import re #regular expressions\n",
    "import logging #verbosity (code telling you what's going on)\n",
    "import csv #data loading\n",
    "import sklearn.ensemble #classifier\n",
    "import sklearn.metrics #performance assessment\n",
    "import sklearn.model_selection #train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading extra data for nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jocelyn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jocelyn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # connectors and other non-semantic rich expressions\n",
    "nltk.download('punkt') # sentence tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some verbosity to the embedding training session, i.e., the coding telling you what is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading spanish stopwords list from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'al',\n",
       " 'algo',\n",
       " 'algunas',\n",
       " 'algunos',\n",
       " 'ante',\n",
       " 'antes',\n",
       " 'como',\n",
       " 'con',\n",
       " 'contra',\n",
       " 'cual',\n",
       " 'cuando',\n",
       " 'de',\n",
       " 'del',\n",
       " 'desde',\n",
       " 'donde',\n",
       " 'durante',\n",
       " 'e',\n",
       " 'el',\n",
       " 'ella',\n",
       " 'ellas',\n",
       " 'ellos',\n",
       " 'en',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'eras',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'esas',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'esos',\n",
       " 'esta',\n",
       " 'estaba',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estabas',\n",
       " 'estad',\n",
       " 'estada',\n",
       " 'estadas',\n",
       " 'estado',\n",
       " 'estados',\n",
       " 'estamos',\n",
       " 'estando',\n",
       " 'estar',\n",
       " 'estaremos',\n",
       " 'estará',\n",
       " 'estarán',\n",
       " 'estarás',\n",
       " 'estaré',\n",
       " 'estaréis',\n",
       " 'estaría',\n",
       " 'estaríais',\n",
       " 'estaríamos',\n",
       " 'estarían',\n",
       " 'estarías',\n",
       " 'estas',\n",
       " 'este',\n",
       " 'estemos',\n",
       " 'esto',\n",
       " 'estos',\n",
       " 'estoy',\n",
       " 'estuve',\n",
       " 'estuviera',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuvieras',\n",
       " 'estuvieron',\n",
       " 'estuviese',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estuvieses',\n",
       " 'estuvimos',\n",
       " 'estuviste',\n",
       " 'estuvisteis',\n",
       " 'estuviéramos',\n",
       " 'estuviésemos',\n",
       " 'estuvo',\n",
       " 'está',\n",
       " 'estábamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'estás',\n",
       " 'esté',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estés',\n",
       " 'fue',\n",
       " 'fuera',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fueras',\n",
       " 'fueron',\n",
       " 'fuese',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'fueses',\n",
       " 'fui',\n",
       " 'fuimos',\n",
       " 'fuiste',\n",
       " 'fuisteis',\n",
       " 'fuéramos',\n",
       " 'fuésemos',\n",
       " 'ha',\n",
       " 'habida',\n",
       " 'habidas',\n",
       " 'habido',\n",
       " 'habidos',\n",
       " 'habiendo',\n",
       " 'habremos',\n",
       " 'habrá',\n",
       " 'habrán',\n",
       " 'habrás',\n",
       " 'habré',\n",
       " 'habréis',\n",
       " 'habría',\n",
       " 'habríais',\n",
       " 'habríamos',\n",
       " 'habrían',\n",
       " 'habrías',\n",
       " 'habéis',\n",
       " 'había',\n",
       " 'habíais',\n",
       " 'habíamos',\n",
       " 'habían',\n",
       " 'habías',\n",
       " 'han',\n",
       " 'has',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'haya',\n",
       " 'hayamos',\n",
       " 'hayan',\n",
       " 'hayas',\n",
       " 'hayáis',\n",
       " 'he',\n",
       " 'hemos',\n",
       " 'hube',\n",
       " 'hubiera',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubieras',\n",
       " 'hubieron',\n",
       " 'hubiese',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'hubieses',\n",
       " 'hubimos',\n",
       " 'hubiste',\n",
       " 'hubisteis',\n",
       " 'hubiéramos',\n",
       " 'hubiésemos',\n",
       " 'hubo',\n",
       " 'la',\n",
       " 'las',\n",
       " 'le',\n",
       " 'les',\n",
       " 'lo',\n",
       " 'los',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'mucho',\n",
       " 'muchos',\n",
       " 'muy',\n",
       " 'más',\n",
       " 'mí',\n",
       " 'mía',\n",
       " 'mías',\n",
       " 'mío',\n",
       " 'míos',\n",
       " 'nada',\n",
       " 'ni',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nosotras',\n",
       " 'nosotros',\n",
       " 'nuestra',\n",
       " 'nuestras',\n",
       " 'nuestro',\n",
       " 'nuestros',\n",
       " 'o',\n",
       " 'os',\n",
       " 'otra',\n",
       " 'otras',\n",
       " 'otro',\n",
       " 'otros',\n",
       " 'para',\n",
       " 'pero',\n",
       " 'poco',\n",
       " 'por',\n",
       " 'porque',\n",
       " 'que',\n",
       " 'quien',\n",
       " 'quienes',\n",
       " 'qué',\n",
       " 'se',\n",
       " 'sea',\n",
       " 'seamos',\n",
       " 'sean',\n",
       " 'seas',\n",
       " 'sentid',\n",
       " 'sentida',\n",
       " 'sentidas',\n",
       " 'sentido',\n",
       " 'sentidos',\n",
       " 'seremos',\n",
       " 'será',\n",
       " 'serán',\n",
       " 'serás',\n",
       " 'seré',\n",
       " 'seréis',\n",
       " 'sería',\n",
       " 'seríais',\n",
       " 'seríamos',\n",
       " 'serían',\n",
       " 'serías',\n",
       " 'seáis',\n",
       " 'siente',\n",
       " 'sin',\n",
       " 'sintiendo',\n",
       " 'sobre',\n",
       " 'sois',\n",
       " 'somos',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'su',\n",
       " 'sus',\n",
       " 'suya',\n",
       " 'suyas',\n",
       " 'suyo',\n",
       " 'suyos',\n",
       " 'sí',\n",
       " 'también',\n",
       " 'tanto',\n",
       " 'te',\n",
       " 'tendremos',\n",
       " 'tendrá',\n",
       " 'tendrán',\n",
       " 'tendrás',\n",
       " 'tendré',\n",
       " 'tendréis',\n",
       " 'tendría',\n",
       " 'tendríais',\n",
       " 'tendríamos',\n",
       " 'tendrían',\n",
       " 'tendrías',\n",
       " 'tened',\n",
       " 'tenemos',\n",
       " 'tenga',\n",
       " 'tengamos',\n",
       " 'tengan',\n",
       " 'tengas',\n",
       " 'tengo',\n",
       " 'tengáis',\n",
       " 'tenida',\n",
       " 'tenidas',\n",
       " 'tenido',\n",
       " 'tenidos',\n",
       " 'teniendo',\n",
       " 'tenéis',\n",
       " 'tenía',\n",
       " 'teníais',\n",
       " 'teníamos',\n",
       " 'tenían',\n",
       " 'tenías',\n",
       " 'ti',\n",
       " 'tiene',\n",
       " 'tienen',\n",
       " 'tienes',\n",
       " 'todo',\n",
       " 'todos',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'tuve',\n",
       " 'tuviera',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuvieras',\n",
       " 'tuvieron',\n",
       " 'tuviese',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'tuvieses',\n",
       " 'tuvimos',\n",
       " 'tuviste',\n",
       " 'tuvisteis',\n",
       " 'tuviéramos',\n",
       " 'tuviésemos',\n",
       " 'tuvo',\n",
       " 'tuya',\n",
       " 'tuyas',\n",
       " 'tuyo',\n",
       " 'tuyos',\n",
       " 'tú',\n",
       " 'un',\n",
       " 'una',\n",
       " 'uno',\n",
       " 'unos',\n",
       " 'vosostras',\n",
       " 'vosostros',\n",
       " 'vuestra',\n",
       " 'vuestras',\n",
       " 'vuestro',\n",
       " 'vuestros',\n",
       " 'y',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'él',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some self-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizer(text): #normalizes a given string to lowercase and changes all vowels to their base form\n",
    "    text = text.lower() #string lowering\n",
    "    text = re.sub(r'[^A-Za-zñáéíóú]', ' ', text) #replaces every punctuation with a space\n",
    "    text = re.sub('á', 'a', text) #replaces special vowels to their base forms\n",
    "    text = re.sub('é', 'e', text)\n",
    "    text = re.sub('í', 'i', text)\n",
    "    text = re.sub('ó', 'o', text)\n",
    "    text = re.sub('ú', 'u', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer(text, model): #returns a vector representation from a list of words and a given model\n",
    "    vectors = []\n",
    "    for i in text:\n",
    "        try:\n",
    "            vectors.append(model.wv[i])\n",
    "        except:\n",
    "            pass\n",
    "    return(np.mean(vectors,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the corpus dataset and taking each line to an array of sentences. We are using the whole Aysen's waiting list as a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open('corpus.txt', encoding='utf-8') as file: #use utf-8 to preserve special characters\n",
    "    for line in file:\n",
    "        sentences.append((line.rstrip())) #use strip to remove \\n (newline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking every sentence and normalizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_sentences = [normalizer(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing every sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in normalized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "without_stopwords_sentences = []\n",
    "for sentence in tokenized_sentences:\n",
    "    without_stopwords_sentence = [word for word in sentence if word not in stopwords]\n",
    "    without_stopwords_sentences.append(without_stopwords_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural word embeddings word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-03 17:32:07,875 : INFO : collecting all words and their counts\n",
      "2019-06-03 17:32:07,878 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-03 17:32:07,895 : INFO : PROGRESS: at sentence #10000, processed 26393 words, keeping 2174 word types\n",
      "2019-06-03 17:32:07,910 : INFO : PROGRESS: at sentence #20000, processed 53049 words, keeping 2795 word types\n",
      "2019-06-03 17:32:07,930 : INFO : PROGRESS: at sentence #30000, processed 80100 words, keeping 3189 word types\n",
      "2019-06-03 17:32:07,953 : INFO : PROGRESS: at sentence #40000, processed 107473 words, keeping 3528 word types\n",
      "2019-06-03 17:32:07,973 : INFO : PROGRESS: at sentence #50000, processed 134970 words, keeping 3783 word types\n",
      "2019-06-03 17:32:08,011 : INFO : PROGRESS: at sentence #60000, processed 162957 words, keeping 3986 word types\n",
      "2019-06-03 17:32:08,037 : INFO : PROGRESS: at sentence #70000, processed 191225 words, keeping 4140 word types\n",
      "2019-06-03 17:32:08,064 : INFO : PROGRESS: at sentence #80000, processed 219291 words, keeping 4298 word types\n",
      "2019-06-03 17:32:08,090 : INFO : PROGRESS: at sentence #90000, processed 247027 words, keeping 4430 word types\n",
      "2019-06-03 17:32:08,116 : INFO : PROGRESS: at sentence #100000, processed 276646 words, keeping 4559 word types\n",
      "2019-06-03 17:32:08,148 : INFO : PROGRESS: at sentence #110000, processed 305141 words, keeping 4645 word types\n",
      "2019-06-03 17:32:08,174 : INFO : PROGRESS: at sentence #120000, processed 335101 words, keeping 4733 word types\n",
      "2019-06-03 17:32:08,203 : INFO : PROGRESS: at sentence #130000, processed 365193 words, keeping 4831 word types\n",
      "2019-06-03 17:32:08,237 : INFO : PROGRESS: at sentence #140000, processed 394265 words, keeping 4894 word types\n",
      "2019-06-03 17:32:08,295 : INFO : PROGRESS: at sentence #150000, processed 423907 words, keeping 4974 word types\n",
      "2019-06-03 17:32:08,325 : INFO : PROGRESS: at sentence #160000, processed 454862 words, keeping 5022 word types\n",
      "2019-06-03 17:32:08,359 : INFO : PROGRESS: at sentence #170000, processed 489295 words, keeping 5075 word types\n",
      "2019-06-03 17:32:08,379 : INFO : PROGRESS: at sentence #180000, processed 523867 words, keeping 5108 word types\n",
      "2019-06-03 17:32:08,397 : INFO : PROGRESS: at sentence #190000, processed 559713 words, keeping 5127 word types\n",
      "2019-06-03 17:32:08,423 : INFO : PROGRESS: at sentence #200000, processed 588689 words, keeping 5147 word types\n",
      "2019-06-03 17:32:08,444 : INFO : PROGRESS: at sentence #210000, processed 608952 words, keeping 5147 word types\n",
      "2019-06-03 17:32:08,459 : INFO : collected 5150 word types from a corpus of 625155 raw words and 217897 sentences\n",
      "2019-06-03 17:32:08,464 : INFO : Loading a fresh vocabulary\n",
      "2019-06-03 17:32:08,489 : INFO : min_count=5 retains 2782 unique words (54% of original 5150, drops 2368)\n",
      "2019-06-03 17:32:08,494 : INFO : min_count=5 leaves 620802 word corpus (99% of original 625155, drops 4353)\n",
      "2019-06-03 17:32:08,521 : INFO : deleting the raw counts dictionary of 5150 items\n",
      "2019-06-03 17:32:08,527 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-06-03 17:32:08,531 : INFO : downsampling leaves estimated 414882 word corpus (66.8% of prior 620802)\n",
      "2019-06-03 17:32:08,549 : INFO : estimated required memory for 2782 words and 100 dimensions: 3616600 bytes\n",
      "2019-06-03 17:32:08,554 : INFO : resetting layer weights\n",
      "2019-06-03 17:32:08,636 : INFO : training model with 3 workers on 2782 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-03 17:32:09,324 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 17:32:09,327 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 17:32:09,329 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 17:32:09,332 : INFO : EPOCH - 1 : training on 625155 raw words (415005 effective words) took 0.7s, 628298 effective words/s\n",
      "2019-06-03 17:32:09,922 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 17:32:09,925 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 17:32:09,932 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 17:32:09,934 : INFO : EPOCH - 2 : training on 625155 raw words (414775 effective words) took 0.6s, 703457 effective words/s\n",
      "2019-06-03 17:32:10,577 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 17:32:10,579 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 17:32:10,581 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 17:32:10,585 : INFO : EPOCH - 3 : training on 625155 raw words (414935 effective words) took 0.6s, 670402 effective words/s\n",
      "2019-06-03 17:32:11,182 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 17:32:11,186 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 17:32:11,190 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 17:32:11,193 : INFO : EPOCH - 4 : training on 625155 raw words (414547 effective words) took 0.6s, 698518 effective words/s\n",
      "2019-06-03 17:32:11,784 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 17:32:11,789 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 17:32:11,791 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 17:32:11,794 : INFO : EPOCH - 5 : training on 625155 raw words (414626 effective words) took 0.6s, 708260 effective words/s\n",
      "2019-06-03 17:32:11,796 : INFO : training on a 3125775 raw words (2073888 effective words) took 3.2s, 657313 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(without_stopwords_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the most similar words to a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-03 17:32:26,294 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('avanzado', 0.8856569528579712),\n",
       " ('metastasis', 0.881154477596283),\n",
       " ('adenocarcinoma', 0.872443437576294),\n",
       " ('infiltrante', 0.8395851850509644),\n",
       " ('adenoma', 0.8390409350395203),\n",
       " ('cervix', 0.8359759449958801),\n",
       " ('fallecido', 0.8250153064727783),\n",
       " ('oseas', 0.8198772668838501),\n",
       " ('padre', 0.8187509775161743),\n",
       " ('significado', 0.8186618089675903)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('cancer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your own words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: Can we classify the medical specialty from the diagnosis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the raw dataset and extracting the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics = [] #classifier raw features\n",
    "specialties = [] #classifier raw labels\n",
    "with open('data.csv', encoding='utf-8') as file:\n",
    "    data = csv.DictReader(file)\n",
    "    for row in data:\n",
    "        diagnostics.append(row['diagnostic'])\n",
    "        specialties.append(row['specialty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_normalized = [normalizer(diagnostic) for diagnostic in diagnostics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_tokenized = [nltk.word_tokenize(diagnostic) for diagnostic in diagnostics_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_wihout_stopwords = []\n",
    "for diagnostic in diagnostics_tokenized:\n",
    "    diagnostic_wihout_stopwords = [word for word in diagnostic if word not in stopwords]\n",
    "    diagnostics_wihout_stopwords.append(diagnostic_wihout_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty matrix to store the encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_matrix = np.zeros((len(diagnostics_wihout_stopwords), len(model.wv['cancer'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the matrix with the vectorized diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for i,diagnostic in enumerate(diagnostics_wihout_stopwords):\n",
    "    vector = vectorizer(diagnostic,model)\n",
    "    diagnostics_matrix[i,] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty vector to store the encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specialties_vector = np.zeros((len(specialties), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the vector with the encoded specialties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,specialty in enumerate(specialties): #OFTALMOLOGIA is encoded as a 1.0 and TRAUMATOLOGIA as a 2.0\n",
    "    if specialty == 'OFTALMOLOGIA':\n",
    "        specialties_vector[i,] = 1\n",
    "    else:\n",
    "        specialties_vector[i,] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating the encoded features ans labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_matrix = np.concatenate([diagnostics_matrix,specialties_vector], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing NAs from the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_matrix_without_nan = data_matrix[~np.isnan(data_matrix).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into training and testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_train, diagnostics_test, specialties_train, specialties_test = sklearn.model_selection.train_test_split(\n",
    "    data_matrix_without_nan[:,:100],\n",
    "    data_matrix_without_nan[:,100],\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = sklearn.ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(diagnostics_train,specialties_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions over the testing subset and measuring their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.78      0.98      0.87      5613\n",
      "        2.0       0.98      0.82      0.89      8542\n",
      "\n",
      "avg / total       0.90      0.88      0.88     14155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(diagnostics_test)\n",
    "print(sklearn.metrics.classification_report(predictions, specialties_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision = TP/(TP+FP) # accuracy of positive predictions \n",
    "- Recall = TP/(TP+FN) # sensitivity or true positive rate\n",
    "- F1-score = 2 x (precision x recall)/(precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual testing of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialtyClassifier(diagnostic):\n",
    "    try:\n",
    "        stringNorm = normalizer(diagnostic)\n",
    "        stringTokenized = nltk.word_tokenize(stringNorm)\n",
    "        stringVec = vectorizer(stringTokenized,model)\n",
    "        result = classifier.predict(stringVec.reshape(1, -1))[0]\n",
    "        if result==1.0: \n",
    "            return('Oftalmología')\n",
    "        if result==2.0: \n",
    "            return('Traumatología')\n",
    "    except:\n",
    "        return(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oftalmología'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specialtyClassifier('vicio de refracción')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traumatología'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specialtyClassifier('cadera')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another embedding model available in gensim that deals well with words that do not appear in the corpus. Let's see if it works well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-03 20:36:59,272 : INFO : collecting all words and their counts\n",
      "2019-06-03 20:36:59,278 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-03 20:36:59,290 : INFO : PROGRESS: at sentence #10000, processed 26393 words, keeping 2174 word types\n",
      "2019-06-03 20:36:59,304 : INFO : PROGRESS: at sentence #20000, processed 53049 words, keeping 2795 word types\n",
      "2019-06-03 20:36:59,317 : INFO : PROGRESS: at sentence #30000, processed 80100 words, keeping 3189 word types\n",
      "2019-06-03 20:36:59,331 : INFO : PROGRESS: at sentence #40000, processed 107473 words, keeping 3528 word types\n",
      "2019-06-03 20:36:59,354 : INFO : PROGRESS: at sentence #50000, processed 134970 words, keeping 3783 word types\n",
      "2019-06-03 20:36:59,369 : INFO : PROGRESS: at sentence #60000, processed 162957 words, keeping 3986 word types\n",
      "2019-06-03 20:36:59,386 : INFO : PROGRESS: at sentence #70000, processed 191225 words, keeping 4140 word types\n",
      "2019-06-03 20:36:59,427 : INFO : PROGRESS: at sentence #80000, processed 219291 words, keeping 4298 word types\n",
      "2019-06-03 20:36:59,444 : INFO : PROGRESS: at sentence #90000, processed 247027 words, keeping 4430 word types\n",
      "2019-06-03 20:36:59,461 : INFO : PROGRESS: at sentence #100000, processed 276646 words, keeping 4559 word types\n",
      "2019-06-03 20:36:59,476 : INFO : PROGRESS: at sentence #110000, processed 305141 words, keeping 4645 word types\n",
      "2019-06-03 20:36:59,529 : INFO : PROGRESS: at sentence #120000, processed 335101 words, keeping 4733 word types\n",
      "2019-06-03 20:36:59,549 : INFO : PROGRESS: at sentence #130000, processed 365193 words, keeping 4831 word types\n",
      "2019-06-03 20:36:59,567 : INFO : PROGRESS: at sentence #140000, processed 394265 words, keeping 4894 word types\n",
      "2019-06-03 20:36:59,580 : INFO : PROGRESS: at sentence #150000, processed 423907 words, keeping 4974 word types\n",
      "2019-06-03 20:36:59,599 : INFO : PROGRESS: at sentence #160000, processed 454862 words, keeping 5022 word types\n",
      "2019-06-03 20:36:59,619 : INFO : PROGRESS: at sentence #170000, processed 489295 words, keeping 5075 word types\n",
      "2019-06-03 20:36:59,637 : INFO : PROGRESS: at sentence #180000, processed 523867 words, keeping 5108 word types\n",
      "2019-06-03 20:36:59,652 : INFO : PROGRESS: at sentence #190000, processed 559713 words, keeping 5127 word types\n",
      "2019-06-03 20:36:59,669 : INFO : PROGRESS: at sentence #200000, processed 588689 words, keeping 5147 word types\n",
      "2019-06-03 20:36:59,679 : INFO : PROGRESS: at sentence #210000, processed 608952 words, keeping 5147 word types\n",
      "2019-06-03 20:36:59,688 : INFO : collected 5150 word types from a corpus of 625155 raw words and 217897 sentences\n",
      "2019-06-03 20:36:59,690 : INFO : Loading a fresh vocabulary\n",
      "2019-06-03 20:36:59,704 : INFO : min_count=5 retains 2782 unique words (54% of original 5150, drops 2368)\n",
      "2019-06-03 20:36:59,707 : INFO : min_count=5 leaves 620802 word corpus (99% of original 625155, drops 4353)\n",
      "2019-06-03 20:36:59,726 : INFO : deleting the raw counts dictionary of 5150 items\n",
      "2019-06-03 20:36:59,729 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-06-03 20:36:59,732 : INFO : downsampling leaves estimated 414882 word corpus (66.8% of prior 620802)\n",
      "2019-06-03 20:37:00,106 : INFO : estimated required memory for 2782 words, 27313 buckets and 100 dimensions: 15338128 bytes\n",
      "2019-06-03 20:37:00,108 : INFO : resetting layer weights\n",
      "2019-06-03 20:37:00,818 : INFO : Total number of ngrams is 27313\n",
      "2019-06-03 20:37:01,244 : INFO : training model with 3 workers on 2782 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-03 20:37:02,453 : INFO : EPOCH 1 - PROGRESS: at 33.55% examples, 134608 words/s, in_qsize 4, out_qsize 1\n",
      "2019-06-03 20:37:03,521 : INFO : EPOCH 1 - PROGRESS: at 57.32% examples, 122263 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-03 20:37:04,522 : INFO : EPOCH 1 - PROGRESS: at 96.59% examples, 127688 words/s, in_qsize 2, out_qsize 1\n",
      "2019-06-03 20:37:04,524 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 20:37:04,530 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 20:37:04,531 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 20:37:04,533 : INFO : EPOCH - 1 : training on 625155 raw words (414836 effective words) took 3.2s, 127976 effective words/s\n",
      "2019-06-03 20:37:05,641 : INFO : EPOCH 2 - PROGRESS: at 36.82% examples, 163018 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-03 20:37:06,681 : INFO : EPOCH 2 - PROGRESS: at 75.46% examples, 168716 words/s, in_qsize 4, out_qsize 1\n",
      "2019-06-03 20:37:07,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 20:37:07,005 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 20:37:07,009 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 20:37:07,011 : INFO : EPOCH - 2 : training on 625155 raw words (414989 effective words) took 2.4s, 171497 effective words/s\n",
      "2019-06-03 20:37:08,065 : INFO : EPOCH 3 - PROGRESS: at 33.55% examples, 155188 words/s, in_qsize 3, out_qsize 2\n",
      "2019-06-03 20:37:09,138 : INFO : EPOCH 3 - PROGRESS: at 71.24% examples, 161520 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-03 20:37:09,554 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 20:37:09,556 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 20:37:09,558 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 20:37:09,560 : INFO : EPOCH - 3 : training on 625155 raw words (414777 effective words) took 2.5s, 165797 effective words/s\n",
      "2019-06-03 20:37:10,687 : INFO : EPOCH 4 - PROGRESS: at 33.55% examples, 147463 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-03 20:37:11,711 : INFO : EPOCH 4 - PROGRESS: at 55.79% examples, 126794 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-03 20:37:12,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 20:37:12,530 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 20:37:12,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 20:37:12,534 : INFO : EPOCH - 4 : training on 625155 raw words (415074 effective words) took 2.9s, 142800 effective words/s\n",
      "2019-06-03 20:37:13,590 : INFO : EPOCH 5 - PROGRESS: at 33.54% examples, 154771 words/s, in_qsize 5, out_qsize 1\n",
      "2019-06-03 20:37:14,617 : INFO : EPOCH 5 - PROGRESS: at 72.74% examples, 167839 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-03 20:37:15,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-03 20:37:15,080 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-03 20:37:15,082 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-03 20:37:15,084 : INFO : EPOCH - 5 : training on 625155 raw words (414771 effective words) took 2.5s, 165741 effective words/s\n",
      "2019-06-03 20:37:15,086 : INFO : training on a 3125775 raw words (2074447 effective words) took 13.8s, 149895 effective words/s\n"
     ]
    }
   ],
   "source": [
    "modelF = FastText(without_stopwords_sentences, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained again the classifier using fastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_matrixF = np.zeros((len(diagnostics_wihout_stopwords), len(model.wv['cancer'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the matrix with the vectorized diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,diagnostic in enumerate(diagnostics_wihout_stopwords):\n",
    "    vector = vectorizer(diagnostic,modelF)\n",
    "    diagnostics_matrixF[i,] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty vector to store the encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specialties_vectorF = np.zeros((len(specialties), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the vector with the encoded specialties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,specialty in enumerate(specialties): #OFTALMOLOGIA is encoded as a 1.0 and TRAUMATOLOGIA as a 2.0\n",
    "    if specialty == 'OFTALMOLOGIA':\n",
    "        specialties_vector[i,] = 1\n",
    "    else:\n",
    "        specialties_vector[i,] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating the encoded features ans labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_matrixF = np.concatenate([diagnostics_matrixF,specialties_vector], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing NAs from the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_matrix_without_nanF = data_matrixF[~np.isnan(data_matrixF).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into training and testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnostics_trainF, diagnostics_testF, specialties_trainF, specialties_testF = sklearn.model_selection.train_test_split(\n",
    "    data_matrix_without_nanF[:,:100],\n",
    "    data_matrix_without_nanF[:,100],\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifierF = sklearn.ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifierF.fit(diagnostics_trainF,specialties_trainF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions over the testing subset and measuring their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.79      0.98      0.87      5681\n",
      "        2.0       0.98      0.82      0.89      8498\n",
      "\n",
      "avg / total       0.90      0.88      0.89     14179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsF = classifierF.predict(diagnostics_testF)\n",
    "print(sklearn.metrics.classification_report(predictionsF, specialties_testF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def specialtyClassifierF(diagnostic):\n",
    "    try:\n",
    "        stringNorm = normalizer(diagnostic)\n",
    "        stringTokenized = nltk.word_tokenize(stringNorm)\n",
    "        stringVec = vectorizer(stringTokenized,modelF)\n",
    "        result = classifierF.predict(stringVec.reshape(1, -1))[0]\n",
    "        if result==1.0: \n",
    "            return('Oftalmología')\n",
    "        if result==2.0: \n",
    "            return('Traumatología')\n",
    "    except:\n",
    "        return(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traumatología'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specialtyClassifier('fractura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traumatología'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specialtyClassifierF('fracturaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-03 20:42:57,481 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-03 20:42:57,495 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('carcinoma', 0.7958588600158691),\n",
       " ('adenocarcinoma', 0.7862062454223633),\n",
       " ('av', 0.7563788890838623),\n",
       " ('cartilago', 0.7142010927200317),\n",
       " ('malagro', 0.7035268545150757),\n",
       " ('masa', 0.7021448612213135),\n",
       " ('campo', 0.7020975351333618),\n",
       " ('microadenoma', 0.6904839277267456),\n",
       " ('cabelludo', 0.6831401586532593),\n",
       " ('adenoma', 0.6822806000709534)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelF.wv.most_similar('cancer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example that different embeddings have different performance, and one can do a qualitative and a quantitative evaluation of models and training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code will not run in the cloud, but you can try more complete embeddings in your computer. \n",
    "\n",
    "For example, you can download the Spanish Billion Word Corpus from https://github.com/dccuchile/spanish-word-embeddings\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('sbw_vectors.bin', binary=True)\n",
    "model.wv.most_similar(\"cancer\", topn=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
