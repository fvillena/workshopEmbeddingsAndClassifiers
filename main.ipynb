{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import logging\n",
    "import csv\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ville\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^A-Za-zñáéíóú]', ' ', text)\n",
    "    text = re.sub('á', 'a', text)\n",
    "    text = re.sub('é', 'e', text)\n",
    "    text = re.sub('í', 'i', text)\n",
    "    text = re.sub('ó', 'o', text)\n",
    "    text = re.sub('ú', 'u', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text, model):\n",
    "    vectors = []\n",
    "    for i in text:\n",
    "        try:\n",
    "            vectors.append(model.wv[i])\n",
    "        except:\n",
    "            pass\n",
    "    return(np.mean(vectors,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open('corpus.txt') as file:\n",
    "    for line in file:\n",
    "        sentences.append((line.rstrip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_sentences = [normalizer(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in normalized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_stopwords_sentences = []\n",
    "for sentence in tokenized_sentences:\n",
    "    without_stopwords_sentence = [word for word in sentence if word not in stopwords]\n",
    "    without_stopwords_sentences.append(without_stopwords_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-02 23:35:18,330 : INFO : collecting all words and their counts\n",
      "2019-06-02 23:35:18,331 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-02 23:35:18,336 : INFO : PROGRESS: at sentence #10000, processed 26393 words, keeping 2174 word types\n",
      "2019-06-02 23:35:18,341 : INFO : PROGRESS: at sentence #20000, processed 53049 words, keeping 2795 word types\n",
      "2019-06-02 23:35:18,346 : INFO : PROGRESS: at sentence #30000, processed 80100 words, keeping 3189 word types\n",
      "2019-06-02 23:35:18,351 : INFO : PROGRESS: at sentence #40000, processed 107473 words, keeping 3528 word types\n",
      "2019-06-02 23:35:18,356 : INFO : PROGRESS: at sentence #50000, processed 134970 words, keeping 3783 word types\n",
      "2019-06-02 23:35:18,361 : INFO : PROGRESS: at sentence #60000, processed 162957 words, keeping 3986 word types\n",
      "2019-06-02 23:35:18,366 : INFO : PROGRESS: at sentence #70000, processed 191225 words, keeping 4140 word types\n",
      "2019-06-02 23:35:18,371 : INFO : PROGRESS: at sentence #80000, processed 219291 words, keeping 4298 word types\n",
      "2019-06-02 23:35:18,377 : INFO : PROGRESS: at sentence #90000, processed 247027 words, keeping 4430 word types\n",
      "2019-06-02 23:35:18,382 : INFO : PROGRESS: at sentence #100000, processed 276646 words, keeping 4559 word types\n",
      "2019-06-02 23:35:18,386 : INFO : PROGRESS: at sentence #110000, processed 305141 words, keeping 4645 word types\n",
      "2019-06-02 23:35:18,391 : INFO : PROGRESS: at sentence #120000, processed 335101 words, keeping 4733 word types\n",
      "2019-06-02 23:35:18,397 : INFO : PROGRESS: at sentence #130000, processed 365193 words, keeping 4831 word types\n",
      "2019-06-02 23:35:18,402 : INFO : PROGRESS: at sentence #140000, processed 394265 words, keeping 4894 word types\n",
      "2019-06-02 23:35:18,407 : INFO : PROGRESS: at sentence #150000, processed 423907 words, keeping 4974 word types\n",
      "2019-06-02 23:35:18,412 : INFO : PROGRESS: at sentence #160000, processed 454862 words, keeping 5022 word types\n",
      "2019-06-02 23:35:18,418 : INFO : PROGRESS: at sentence #170000, processed 489295 words, keeping 5075 word types\n",
      "2019-06-02 23:35:18,423 : INFO : PROGRESS: at sentence #180000, processed 523867 words, keeping 5108 word types\n",
      "2019-06-02 23:35:18,429 : INFO : PROGRESS: at sentence #190000, processed 559713 words, keeping 5127 word types\n",
      "2019-06-02 23:35:18,434 : INFO : PROGRESS: at sentence #200000, processed 588689 words, keeping 5147 word types\n",
      "2019-06-02 23:35:18,438 : INFO : PROGRESS: at sentence #210000, processed 608952 words, keeping 5147 word types\n",
      "2019-06-02 23:35:18,441 : INFO : collected 5150 word types from a corpus of 625155 raw words and 217897 sentences\n",
      "2019-06-02 23:35:18,441 : INFO : Loading a fresh vocabulary\n",
      "2019-06-02 23:35:18,445 : INFO : effective_min_count=5 retains 2782 unique words (54% of original 5150, drops 2368)\n",
      "2019-06-02 23:35:18,445 : INFO : effective_min_count=5 leaves 620802 word corpus (99% of original 625155, drops 4353)\n",
      "2019-06-02 23:35:18,450 : INFO : deleting the raw counts dictionary of 5150 items\n",
      "2019-06-02 23:35:18,450 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-06-02 23:35:18,450 : INFO : downsampling leaves estimated 414882 word corpus (66.8% of prior 620802)\n",
      "2019-06-02 23:35:18,454 : INFO : estimated required memory for 2782 words and 100 dimensions: 3616600 bytes\n",
      "2019-06-02 23:35:18,454 : INFO : resetting layer weights\n",
      "2019-06-02 23:35:18,488 : INFO : training model with 3 workers on 2782 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-02 23:35:18,720 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-02 23:35:18,721 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-02 23:35:18,722 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-02 23:35:18,722 : INFO : EPOCH - 1 : training on 625155 raw words (414735 effective words) took 0.2s, 1840511 effective words/s\n",
      "2019-06-02 23:35:18,952 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-02 23:35:18,952 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-02 23:35:18,953 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-02 23:35:18,953 : INFO : EPOCH - 2 : training on 625155 raw words (414744 effective words) took 0.2s, 1847240 effective words/s\n",
      "2019-06-02 23:35:19,186 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-02 23:35:19,187 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-02 23:35:19,188 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-02 23:35:19,188 : INFO : EPOCH - 3 : training on 625155 raw words (414605 effective words) took 0.2s, 1875045 effective words/s\n",
      "2019-06-02 23:35:19,434 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-02 23:35:19,435 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-02 23:35:19,436 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-02 23:35:19,436 : INFO : EPOCH - 4 : training on 625155 raw words (415043 effective words) took 0.2s, 1734692 effective words/s\n",
      "2019-06-02 23:35:19,672 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-02 23:35:19,673 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-02 23:35:19,673 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-02 23:35:19,673 : INFO : EPOCH - 5 : training on 625155 raw words (414785 effective words) took 0.2s, 1815000 effective words/s\n",
      "2019-06-02 23:35:19,674 : INFO : training on a 3125775 raw words (2073912 effective words) took 1.2s, 1750200 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(without_stopwords_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-06-02 23:35:24,679 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('avanzado', 0.884432315826416),\n",
       " ('fallecido', 0.8637561202049255),\n",
       " ('adenocarcinoma', 0.862040638923645),\n",
       " ('metastasis', 0.8620170950889587),\n",
       " ('adenoma', 0.8476477861404419),\n",
       " ('infiltrante', 0.8386375904083252),\n",
       " ('cervix', 0.8291419744491577),\n",
       " ('indeterminado', 0.8254727721214294),\n",
       " ('significado', 0.8203879594802856),\n",
       " ('fundus', 0.8197342157363892)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics = []\n",
    "specialties = []\n",
    "with open('data.csv', encoding='utf-8') as file:\n",
    "    data = csv.DictReader(file)\n",
    "    for row in data:\n",
    "        diagnostics.append(row['diagnostic'])\n",
    "        specialties.append(row['specialty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_normalized = [normalizer(diagnostic) for diagnostic in diagnostics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_tokenized = [nltk.word_tokenize(diagnostic) for diagnostic in diagnostics_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_wihout_stopwords = []\n",
    "for diagnostic in diagnostics_tokenized:\n",
    "    diagnostic_wihout_stopwords = [word for word in diagnostic if word not in stopwords]\n",
    "    diagnostics_wihout_stopwords.append(diagnostic_wihout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "diagnostics_matrix = np.zeros((len(diagnostics_wihout_stopwords), len(model['cancer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for i,diagnostic in enumerate(diagnostics_wihout_stopwords):\n",
    "    vector = vectorizer(diagnostic,model)\n",
    "    diagnostics_matrix[i,] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialties_vector = np.zeros((len(specialties), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,specialty in enumerate(specialties):\n",
    "    if specialty == 'OFTALMOLOGIA':\n",
    "        specialties_vector[i,] = 1\n",
    "    else:\n",
    "        specialties_vector[i,] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.concatenate([diagnostics_matrix,specialties_vector], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_without_nan = data_matrix[~np.isnan(data_matrix).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_train, diagnostics_test, specialties_train, specialties_test = sklearn.model_selection.train_test_split(\n",
    "    data_matrix_without_nan[:,:100],\n",
    "    data_matrix_without_nan[:,100],\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(diagnostics_train,specialties_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.98      0.87      5621\n",
      "         2.0       0.98      0.82      0.89      8534\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     14155\n",
      "   macro avg       0.88      0.90      0.88     14155\n",
      "weighted avg       0.90      0.88      0.88     14155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(diagnostics_test)\n",
    "print(sklearn.metrics.classification_report(predictions, specialties_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialtyClassifier(diagnostic):\n",
    "    try:\n",
    "        stringNorm = normalizer(diagnostic)\n",
    "        stringTokenized = nltk.word_tokenize(stringNorm)\n",
    "        stringVec = vectorizer(stringTokenized,model)\n",
    "        result = classifier.predict(stringVec.reshape(1, -1))[0]\n",
    "        return(result)\n",
    "    except:\n",
    "        return(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specialtyClassifier('vicio de refracción')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
